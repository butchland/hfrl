[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HuggingFace Reinforcement Learning",
    "section": "",
    "text": "My notes for the Hugging Face Reinforcement Learning Course"
  },
  {
    "objectID": "index.html#notes",
    "href": "index.html#notes",
    "title": "HuggingFace Reinforcement Learning",
    "section": "Notes",
    "text": "Notes\n\nUnit 1 (Intro) - partial"
  },
  {
    "objectID": "unit1.html",
    "href": "unit1.html",
    "title": "Unit 1",
    "section": "",
    "text": "What is RL\n\nbig picture:\n\nAgent(AI) will Learn from Environment by Interacting with Environment and receive Rewards (neg/pos) as Feedback for performing Actions\n while not done:    \n     Environment -> observation -> Agent \n    Agent -> action -> Environment\n    Environment -> reward, new observation, done -> Agent\n\nexample:\n\n     boy learning to play video game \n         - boy is Agent,\n           screen is observation\n           buttons is set of possible actions \n           game is environment\n           rewards is points\n\n         - boy observes screen (sees avatar, coin, squid)      \n         - boy presses right button \n         - game reacts by updating screen -> avatar touches coin, rewards point\n              - boy learns getting coin rewards +1 \n              - boy presses button again and tough\n              - game reacts by updating screen -> avatar touches squid and dies, rewarding neg points (dead) \n              - boy learns touching squid rewards dead, and ends game\n\nformal definition:\n\nRL is FRAMEWORK for solving CONTROL TASKS (aka decision problems)\nby building AGENTS\nAGENTS that LEARN from the ENVIRONMENT\n\nLEARN by INTERACTING with ENVIRONMENT\nLEARN thru TRIAL and ERROR\nLEARN by receiving REWARDS(NEGATIVE/POSITIVE) as FEEDBACK\n\n\n\nRL Framework\n\nRL Process\n\nEnvironment \\to (state\\, S_{t}, reward\\, R_{t}) \\to Agent \\to action\\, A_{t} \\to Environment \\to (state\\, S_{t+1}, reward\\, R_{t+1}) \\to Agent (loop)\nIOW: S_0 \\to A_0 \\to (R_1,S_1) \\to A_1 \\to (R_2, S_2) \\to A_2 \\to ... (R_n, S_n) \\to A_n \\to (R_{n+1}, S_{n+1}) \\to...\nAgent’s GOAL: Maximize its CUMULATIVE REWARD aka EXPECTED RETURN\nWHY IS THIS(Maximization of EXPECTED RETURN) the Agent’s goal? b/c RL is based the REWARD HYPOTHESIS\n\nthe central idea of RL: The reward hypothesis\n\nALL GOALS can be described as the MAXIMIZATION of EXPECTED RETURN\n\nExample?\n\nTO HAVE BEST BEHAVIOR, MAXIMIZE the EXPECTED CUMULATIVE REWARD\n\nare EXPECTED CUMULATIVE REWARD same as EXPECTED RETURN? ==> YES!\n\n\nANOTHER NAME for RL Process: MARKOV DECISION PROCESS (MDP)\nMarkov property:\n\nAGENT only needs CURRENT STATE to decide what ACTION to TAKE\nCONTRAST with NEEDING HISTORY of ALL STATES and ACTIONS they took before\n\nObservations/States Space\n\nObservations/States - information our AGENT gets from the ENVIRONMENT.\n\ne.g. for a videogame -> Observation/State is a Screenshot (AKA Frame)\n\nDifference between State vs Observation:\n\nState: complete description of Environment (IOW, no hidden information) e.g. CHESS BOARD is STATE (AKA FULLY OBSERVED ENVIRONMENT)\nObservation: partial description of the Environment/State (e.g. Super Mario Bros shows part of screen close to Agent but not entire level)\n\n\nAction Space\n\nset of all possible actions in the environment\ntypes: Discrete or Continuous\nDiscrete: number of actions is finite\n\nexample: Supermario actions are limited to 4 directions + jump\n\nContinuous: number of actions is INFINITE\n\nexample: self-driving car has unlimited gradation in angles in directions + other actions\n\naction space is crucial - plays important role in choosing the RL algo\n\nRewards and the discounting:\n\nreward is fundamental in RL:\n\nits the only feedback for the agent\nits the only way the agent knows if the action taken was good or not.\n\nCumulative reward at each step t is R(\\tau) = r_{t+1} + r_{t+2} + r_{t+3} + r_{t+4} + ...\n\nTrajectory \\tau (read Tau) - sequence of states and actions\nReturn R(\\tau) : cumulative reward is equal to sum of all rewards of the sequence\n\nEquivalent to  R(\\tau) = \\sum_{k=0}^{\\infty} r_{t + k + 1}\nDiscounting:\n\nrewards that come sooner are more likely to happen since they are more predictable than the long term future rewards\nDefine a discount rate gamma between 0 and 1. Most of the time its within (0.99, 0.95)\n\nthe larger the gamma, the smaller the discount (long term is given more importance)\nthe smaller the gamma, the bigger the discount (short term is given more importance)\n\nThen, each reward will be discounted by gamma to exponent of the time step. As the time step increases, the future reward is less and less likely to happen.\nDiscounted cumulative expected reward is R(\\tau) = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\gamma^3 r_{t+4} + ...\n\n\\gamma : (gamma) discount rate\n\\tau : (tau) trajectory (i.e. sequence of states and actions)\nR(\\tau) : (Return) cumulative reward\n\n\n\nTask Types\n\ntask: instance of RL problem.\nepisodic task - has a starting and ending point (aka terminal state). An episode is a list of States, Actions, Rewards and new States.\n\nexample: Super Mario Bros - episode begins at the launch of a new level and terminates when character is killed or reaches the end of the level.\n\ncontinuous task - tasks has no terminal state. Agent must learn to how to choose the best action and simultaneously interact with environment.\n\nexample: stock trading bot: no starting and terminal state – bot continues running until we decide to stop it.\n\n\n\nExploration/Exploitation Tradeoff\n\nExploration is exploring the environment by trying RANDOM actions in order to find more information about the environment\nExploitation is using known information to MAXIMIZE the reward\nGoal of the Agent is maximize the reward but if agent doesn’t explore, it can only exploit nearby states. If the agent explores the enviroment, it can discover bigger rewards.\nAnalogy: local minima - agent explores nearby areas and doesn’t find a better global minima without exploration outside of local minima\nNeed to balance tradeoff between exploitation (maximization of KNOWN rewards) vs exploration (discover NEW, possible BIGGER rewards)\nNeed to define a RULE to handle trade-off\n\nSolving RL Problems: 2 main approaches\n\nHow to build an RL Agent that can SELECT ACTIONS that MAXIMIZE its EXPECTED CUMULATIVE REWARD (aka Return R(\\tau))\nThe Policy PI \\pi : The Agent’s Brain\n\nthe function that tells us the action to take given the state we are in.  State \\to \\pi(State) \\to Action\nThis Policy \\pi is the function we WANT to LEARN; our GOAL is to FIND the OPTIMAL POLICY – the POLICY that MAXIMIZES the EXPECTED RETURN when the AGENT acts according to it.\nWe find this POLICY through training.\n\nTwo Approaches to find POLICY \\pi:\n\nPolicy based Methods - directly; teach agent to learn which action to take given the state is in\n\nValue based Methods - indirectly; teach agent to learn which state is more valuable then take action that leads to more valuable states\n\nTwo Types of Policy:\n\nDeterministic : Policy at a given state will ALWAYS return the same action \na = \\pi (s)\n action = policy(state)\nStochastic: Policy outputs a PROBALITY DISTRIBUTION over actions \n\\pi (a | s )  = P[A | s ]\n policy(actions|state) = probability distribution over the set of actions given the current state\n\nValue based Methods: Learn a VALUE FUNCTION that maps a STATE to the EXPECTED VALUE of being at that STATE\n\nthe VALUE of a state is the EXPECTED DISCOUNTED RETURN the agent can get if it starts in that state and then ACT according to our policy. \nv_{\\pi} (s) = \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + \\dots | S_{t} = s]\n\nAct according to our policy - going to the state with the highest value\n\n\n“Deep” in Deep RL\n\nApplication of Deep Neural Nets to solve RL Problems\nExample: Value based Algorithms\n\nQ-Learning (classic RL) to create a Q table to lookup the action for each state\nvs. Deep Q-Learning - use a Neural Net (NN) to approximate the Q value"
  }
]